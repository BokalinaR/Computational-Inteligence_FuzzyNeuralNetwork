{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics as met\n",
    "\n",
    "from fuzzy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuzzyKNN:\n",
    "    def __init__(self, sensitivity, exp_bound):\n",
    "        # konstruktor\n",
    "        self.sensitivity = sensitivity\n",
    "        self.hyperboxes = None\n",
    "        self.classes = np.array([])\n",
    "        self.exp_bound = exp_bound\n",
    "        \n",
    "    def membership(self, pattern):\n",
    "        # racuna pripadnost i vraca niz pripadnosti svakom hiperboksu\n",
    "        min_pts = self.hyperboxes[:, 0, :]\n",
    "        max_pts = self.hyperboxes[:, 1, :]\n",
    "\n",
    "        a = np.maximum(0, (1 - np.maximum(0, (self.sensitivity * np.minimum(1, pattern - max_pts)))))\n",
    "        b = np.maximum(0, (1 - np.maximum(0, (self.sensitivity * np.minimum(1, min_pts - pattern)))))\n",
    "        \n",
    "        return np.sum(a + b, axis=1) / (2 * len(pattern))\n",
    "    \n",
    "    def overlap_contract(self, index):\n",
    "        #proveravamo da li se hiperboksevi preklapaju\n",
    "        contracted = False\n",
    "        for test_box in range(len(self.hyperboxes)):\n",
    "            if self.classes[test_box] == self.classes[index]:\n",
    "                # ignorisemo preklapanje hiperbokseva iste klase\n",
    "                continue\n",
    "            expanded_box = self.hyperboxes[index]\n",
    "            box = self.hyperboxes[test_box]\n",
    "            \n",
    "            vj, wj = expanded_box #onaj za koji gledamo da li se preklapa sa nekim\n",
    "            vk, wk = box\n",
    "\n",
    "            # moguci slucajevi preklapanja\n",
    "            # trazimo najmanje preklapanje\n",
    "            delta_new = delta_old = 1\n",
    "            min_overlap_index = -1\n",
    "            for i in range(len(vj)):\n",
    "                if vj[i] < vk[i] < wj[i] < wk[i]:\n",
    "                    delta_new = min(delta_old, wj[i] - vk[i])\n",
    "                elif vk[i] < vj[i] < wk[i] < wj[i]:\n",
    "                    delta_new = min(delta_old, wk[i] - vj[i])\n",
    "                \n",
    "                elif vj[i] < vk[i] < wk[i] < wj[i]:\n",
    "                    delta_new = min(delta_old, min(wj[i] - vk[i], wk[i] - vj[i]))\n",
    "\n",
    "                elif vk[i] < vj[i] < wj[i] < wk[i]:\n",
    "                    delta_new = min(delta_old, min(wj[i] - vk[i], wk[i] - vj[i]))\n",
    "\n",
    "                if delta_old - delta_new > 0:\n",
    "                    min_overlap_index = i\n",
    "                    delta_old = delta_new\n",
    "\n",
    "            # ako ima preklapanja, \n",
    "            # gledamo po kojoj strani smanjujemo hiperbokseve\n",
    "            if min_overlap_index >= 0:\n",
    "                i = min_overlap_index\n",
    "                if vj[i] < vk[i] < wj[i] < wk[i]:\n",
    "                    vk[i] = wj[i] = (vk[i] + wj[i])/2\n",
    "\n",
    "                elif vk[i] < vj[i] < wk[i] < wj[i]:\n",
    "                    vj[i] = wk[i] = (vj[i] + wk[i])/2\n",
    "\n",
    "                elif vj[i] < vk[i] < wk[i] < wj[i]:\n",
    "                    if (wj[i] - vk[i]) > (wk[i] - vj[i]):\n",
    "                        vj[i] = wk[i]\n",
    "\n",
    "                    else:\n",
    "                        wj[i] = vk[i]\n",
    "\n",
    "                elif vk[i] < vj[i] < wj[i] < wk[i]:\n",
    "                    if (wk[i] - vj[i]) > (wj[i] - vk[i]):\n",
    "                        vk[i] = wj[i]\n",
    "\n",
    "                    else:\n",
    "                        wk[i] = vj[i]\n",
    "\n",
    "                self.hyperboxes[test_box] = np.array([vk, wk])\n",
    "                self.hyperboxes[index] = np.array([vj, wj])\n",
    "                contracted = True\n",
    "                \n",
    "        return contracted\n",
    "    \n",
    "    def train_pattern(self, X, Y):\n",
    "        # funkcija koja trenira klasifikator\n",
    "        target = Y\n",
    "        \n",
    "        # ako nemamo tu klasu u klasama\n",
    "        if target not in self.classes:\n",
    "            # pravimo hiperboks\n",
    "            if self.hyperboxes is not None:\n",
    "                self.hyperboxes = np.vstack((self.hyperboxes, np.array([[X, X]])))\n",
    "                #print('dodao sam hiperboks: ', self.hyperboxes)\n",
    "                self.classes = np.hstack((self.classes, np.array([target])))\n",
    "                #print('dodao sam klasu: ', self.classes)\n",
    "\n",
    "            else:\n",
    "                self.hyperboxes = np.array([[X, X]])\n",
    "                self.classes = np.array([target])\n",
    "        else:\n",
    "\n",
    "            # sortiramo pripadnosti svim hiperboksevima za trazenu klasu\n",
    "            memberships = self.membership(X)\n",
    "            #print('memberships1: ', memberships)\n",
    "            memberships[np.where(self.classes != target)] = 0\n",
    "            #print('memberships2: ', memberships)\n",
    "            memberships = sorted(list(enumerate(memberships)), key=lambda x: x[1], reverse=True)\n",
    "            #print('memberships3: ', memberships)\n",
    "            \n",
    "            # Sirimo hiperboks\n",
    "            count = 0\n",
    "            while True:\n",
    "                index = memberships[count][0]\n",
    "                min_new = np.minimum(self.hyperboxes[index, 0, :], X)\n",
    "                max_new = np.maximum(self.hyperboxes[index, 1, :], X)\n",
    "                \n",
    "                if self.exp_bound * len(np.unique(self.classes)) >= np.sum(max_new - min_new):\n",
    "                    self.hyperboxes[index, 0] = min_new\n",
    "                    self.hyperboxes[index, 1] = max_new\n",
    "                    break\n",
    "                else:\n",
    "                    count += 1\n",
    "\n",
    "                if count == len(memberships):\n",
    "                    self.hyperboxes = np.vstack((self.hyperboxes, np.array([[X, X]])))\n",
    "                    self.classes = np.hstack((self.classes, np.array([target])))\n",
    "                    index = len(self.hyperboxes) - 1\n",
    "                    break\n",
    "                    \n",
    "            contracted = self.overlap_contract(index)\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        for x, y in zip(X, Y):\n",
    "            self.train_pattern(x, y)\n",
    "\n",
    "    \n",
    "    # predvidjamo klasu\n",
    "    def predict(self, X, k):\n",
    "        \n",
    "        #uzimamo tacke koje odredjuju hiperbokseve\n",
    "        min_pts = self.hyperboxes[:, 0, :]\n",
    "        max_pts = self.hyperboxes[:, 1, :]\n",
    "      \n",
    "        # broj klasa \n",
    "        # i niz u kome cemo brojati pojavljivanje svake klase\n",
    "        n_classes = len(np.unique(self.classes))\n",
    "        cl = np.zeros(n_classes)\n",
    "        \n",
    "        # racunamo udaljenost tacke X od svakog hiperboksa\n",
    "        # tako sto racunamo udaljenost X od prave koja prolazi kroz tacke koje odredjuju hiperboks\n",
    "        distance = []\n",
    "        for i in range(len(min_pts)):\n",
    "            x1 = min_pts[i][0]\n",
    "            y1 = min_pts[i][1]\n",
    "            x2 = max_pts[i][0]\n",
    "            y2 = max_pts[i][1]\n",
    "            \n",
    "            if(x1 == x2 and y1 == y2):\n",
    "                d = abs(math.sqrt((x1-X[0])**2 + (y1 - X[1])**2))\n",
    "                distance.append(d)\n",
    "            elif(x1 == x2):\n",
    "                d = min(abs(math.sqrt((x1-X[0])**2 + (y1 - X[1])**2)), \n",
    "                       abs(math.sqrt((x2-X[0])**2 + (y2 - X[1])**2)))\n",
    "                distance.append(d)\n",
    "            elif(y1 == y2):\n",
    "                d = min(abs(math.sqrt((x1-X[0])**2 + (y1 - X[1])**2)), \n",
    "                       abs(math.sqrt((x2-X[0])**2 + (y2 - X[1])**2)))\n",
    "                distance.append(d)\n",
    "            else:\n",
    "               # print('(x1, y1): ', x1, \" \", y1, end=\"\\n\")\n",
    "               # print('(x2, y2): ', x2, \" \", y2, end=\"\\n\")\n",
    "                d = abs((y2-y1)*X[0] - (x2-x1)*X[1] + x2*y1 - y2*x1) / math.sqrt((y2-y1)**2 + (x2-x1)**2)\n",
    "                distance.append(d)\n",
    "\n",
    "        # sortiramo udaljenosti od najmanje ka najvecoj\n",
    "        # i uzimamo prvih k najblizih hiperbokseva\n",
    "        distance = sorted(list(enumerate(distance)), key=lambda x: x[1])\n",
    "        #print('distance: ', distance, end='\\n')\n",
    "        distance = distance[:k]\n",
    "        #print('distance_k: ', distance, end='\\n')\n",
    "        distance_index = []\n",
    "        for i in range(len(distance)):\n",
    "            distance_index.append(distance[i][0])\n",
    "            \n",
    "        # brojimo pojavljivanje svake klase na osnovu hiperboksa koji joj pripada\n",
    "        for i in range(len(distance_index)):\n",
    "            index = distance_index[i]\n",
    "            _class = self.classes[index]\n",
    "            cl[_class] += 1\n",
    "            #print('cl: ', cl, end = \"\\n\")\n",
    "        \n",
    "        # nalazimo najbrojniju klasu koja je konacna klasa X\n",
    "        max = 0\n",
    "        final_class = 0\n",
    "        for i in range (len(cl)):\n",
    "            if(cl[i] >= max):\n",
    "                max = cl[i]\n",
    "                final_class = i\n",
    "        \n",
    "        \n",
    "        return final_class\n",
    "       \n",
    "    # funkcija koja racuna procenat uspesno klasifikovanih instanci\n",
    "    def score(self, X, Y, k):\n",
    "        count = 0\n",
    "        for x, y in zip(X, Y):\n",
    "            pred = self.predict(x, k)\n",
    "            if y == pred:\n",
    "                count += 1\n",
    "        print(count)\n",
    "        print(len(Y))\n",
    "        \n",
    "        return count / len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sepal length  sepal width  petal length  petal width  class\n",
      "96           5.7          2.9           4.2          1.3      1\n",
      "86           6.7          3.1           4.7          1.5      1\n",
      "37           4.9          3.1           1.5          0.1      0\n",
      "60           5.0          2.0           3.5          1.0      1\n",
      "49           5.0          3.3           1.4          0.2      0\n",
      "79           5.7          2.6           3.5          1.0      1\n",
      "78           6.0          2.9           4.5          1.5      1\n",
      "73           6.1          2.8           4.7          1.2      1\n",
      "77           6.7          3.0           5.0          1.7      1\n",
      "19           5.1          3.8           1.5          0.3      0\n",
      "26           5.0          3.4           1.6          0.4      0\n",
      "58           6.6          2.9           4.6          1.3      1\n",
      "5            5.4          3.9           1.7          0.4      0\n",
      "33           5.5          4.2           1.4          0.2      0\n",
      "80           5.5          2.4           3.8          1.1      1\n",
      "28           5.2          3.4           1.4          0.2      0\n",
      "53           5.5          2.3           4.0          1.3      1\n",
      "16           5.4          3.9           1.3          0.4      0\n",
      "1            4.9          3.0           1.4          0.2      0\n",
      "74           6.4          2.9           4.3          1.3      1\n",
      "13           4.3          3.0           1.1          0.1      0\n",
      "57           4.9          2.4           3.3          1.0      1\n",
      "94           5.6          2.7           4.2          1.3      1\n",
      "22           4.6          3.6           1.0          0.2      0\n",
      "24           4.8          3.4           1.9          0.2      0\n",
      "48           5.3          3.7           1.5          0.2      0\n",
      "51           6.4          3.2           4.5          1.5      1\n",
      "4            5.0          3.6           1.4          0.2      0\n",
      "91           6.1          3.0           4.6          1.4      1\n",
      "8            4.4          2.9           1.4          0.2      0\n",
      "..           ...          ...           ...          ...    ...\n",
      "99           5.7          2.8           4.1          1.3      1\n",
      "36           5.5          3.5           1.3          0.2      0\n",
      "84           5.4          3.0           4.5          1.5      1\n",
      "27           5.2          3.5           1.5          0.2      0\n",
      "67           5.8          2.7           4.1          1.0      1\n",
      "45           4.8          3.0           1.4          0.3      0\n",
      "98           5.1          2.5           3.0          1.1      1\n",
      "83           6.0          2.7           5.1          1.6      1\n",
      "64           5.6          2.9           3.6          1.3      1\n",
      "15           5.7          4.4           1.5          0.4      0\n",
      "20           5.4          3.4           1.7          0.2      0\n",
      "70           5.9          3.2           4.8          1.8      1\n",
      "68           6.2          2.2           4.5          1.5      1\n",
      "65           6.7          3.1           4.4          1.4      1\n",
      "11           4.8          3.4           1.6          0.2      0\n",
      "39           5.1          3.4           1.5          0.2      0\n",
      "25           5.0          3.0           1.6          0.2      0\n",
      "52           6.9          3.1           4.9          1.5      1\n",
      "18           5.7          3.8           1.7          0.3      0\n",
      "35           5.0          3.2           1.2          0.2      0\n",
      "88           5.6          3.0           4.1          1.3      1\n",
      "46           5.1          3.8           1.6          0.2      0\n",
      "90           5.5          2.6           4.4          1.2      1\n",
      "63           6.1          2.9           4.7          1.4      1\n",
      "85           6.0          3.4           4.5          1.6      1\n",
      "81           5.5          2.4           3.7          1.0      1\n",
      "93           5.0          2.3           3.3          1.0      1\n",
      "54           6.5          2.8           4.6          1.5      1\n",
      "41           4.5          2.3           1.3          0.3      0\n",
      "10           5.4          3.7           1.5          0.2      0\n",
      "\n",
      "[100 rows x 5 columns]\n",
      "[[0.2962963  0.09756098]\n",
      " [0.22222222 0.09756098]\n",
      " [0.14814815 0.07317073]\n",
      " [0.11111111 0.12195122]\n",
      " [0.25925926 0.09756098]\n",
      " [0.40740741 0.17073171]\n",
      " [0.11111111 0.09756098]\n",
      " [0.25925926 0.12195122]\n",
      " [0.03703704 0.09756098]\n",
      " [0.22222222 0.12195122]\n",
      " [0.40740741 0.12195122]\n",
      " [0.18518519 0.14634146]\n",
      " [0.18518519 0.09756098]\n",
      " [0.         0.02439024]\n",
      " [0.55555556 0.04878049]\n",
      " [0.51851852 0.12195122]\n",
      " [0.40740741 0.07317073]\n",
      " [0.2962963  0.09756098]\n",
      " [0.51851852 0.17073171]\n",
      " [0.2962963  0.12195122]\n",
      " [0.40740741 0.17073171]\n",
      " [0.2962963  0.12195122]\n",
      " [0.11111111 0.        ]\n",
      " [0.2962963  0.17073171]\n",
      " [0.18518519 0.2195122 ]\n",
      " [0.25925926 0.14634146]\n",
      " [0.25925926 0.14634146]\n",
      " [0.33333333 0.12195122]\n",
      " [0.33333333 0.09756098]\n",
      " [0.14814815 0.14634146]\n",
      " [0.18518519 0.14634146]\n",
      " [0.40740741 0.12195122]\n",
      " [0.33333333 0.12195122]\n",
      " [0.44444444 0.09756098]\n",
      " [0.22222222 0.12195122]\n",
      " [0.25925926 0.04878049]\n",
      " [0.44444444 0.07317073]\n",
      " [0.22222222 0.12195122]\n",
      " [0.03703704 0.07317073]\n",
      " [0.2962963  0.12195122]\n",
      " [0.25925926 0.07317073]\n",
      " [0.07407407 0.07317073]\n",
      " [0.03703704 0.07317073]\n",
      " [0.25925926 0.14634146]\n",
      " [0.2962963  0.2195122 ]\n",
      " [0.18518519 0.09756098]\n",
      " [0.2962963  0.14634146]\n",
      " [0.11111111 0.09756098]\n",
      " [0.37037037 0.12195122]\n",
      " [0.25925926 0.09756098]\n",
      " [1.         0.90243902]\n",
      " [0.77777778 0.85365854]\n",
      " [0.96296296 0.95121951]\n",
      " [0.44444444 0.73170732]\n",
      " [0.81481481 0.87804878]\n",
      " [0.51851852 0.85365854]\n",
      " [0.74074074 0.90243902]\n",
      " [0.22222222 0.56097561]\n",
      " [0.85185185 0.87804878]\n",
      " [0.33333333 0.70731707]\n",
      " [0.25925926 0.6097561 ]\n",
      " [0.59259259 0.7804878 ]\n",
      " [0.62962963 0.73170732]\n",
      " [0.66666667 0.90243902]\n",
      " [0.48148148 0.63414634]\n",
      " [0.88888889 0.82926829]\n",
      " [0.48148148 0.85365854]\n",
      " [0.55555556 0.75609756]\n",
      " [0.7037037  0.85365854]\n",
      " [0.48148148 0.70731707]\n",
      " [0.59259259 0.92682927]\n",
      " [0.66666667 0.73170732]\n",
      " [0.74074074 0.95121951]\n",
      " [0.66666667 0.90243902]\n",
      " [0.77777778 0.80487805]\n",
      " [0.85185185 0.82926829]\n",
      " [0.92592593 0.92682927]\n",
      " [0.88888889 0.97560976]\n",
      " [0.62962963 0.85365854]\n",
      " [0.51851852 0.6097561 ]\n",
      " [0.44444444 0.68292683]\n",
      " [0.44444444 0.65853659]\n",
      " [0.55555556 0.70731707]\n",
      " [0.62962963 1.        ]\n",
      " [0.40740741 0.85365854]\n",
      " [0.62962963 0.85365854]\n",
      " [0.88888889 0.90243902]\n",
      " [0.74074074 0.82926829]\n",
      " [0.48148148 0.75609756]\n",
      " [0.44444444 0.73170732]\n",
      " [0.44444444 0.82926829]\n",
      " [0.66666667 0.87804878]\n",
      " [0.55555556 0.73170732]\n",
      " [0.25925926 0.56097561]\n",
      " [0.48148148 0.7804878 ]\n",
      " [0.51851852 0.7804878 ]\n",
      " [0.51851852 0.7804878 ]\n",
      " [0.7037037  0.80487805]\n",
      " [0.2962963  0.48780488]\n",
      " [0.51851852 0.75609756]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('iris.data', header=None, names=['sepal length', 'sepal width', 'petal length', 'petal width', 'class'])\n",
    "df = df[~(df['class']=='Iris-virginica')]\n",
    "#df.head()\n",
    "\n",
    "df.replace(to_replace='Iris-setosa', value=0, inplace=True)\n",
    "\n",
    "df.replace(to_replace='Iris-versicolor', value=1, inplace=True)\n",
    "\n",
    "print(df.sample(frac=1))\n",
    "\n",
    "X_train = df[['sepal length', 'petal length']].values\n",
    "Y_train = df['class'].values\n",
    "\n",
    "_max = np.max(X_train, axis=0)\n",
    "_min = np.min(X_train, axis=0)\n",
    "X_train = (X_train - _min) / (_max - _min)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, Y_test = X_train[-20:], Y_train[-20:]\n",
    "X_train, Y_train = X_train[:-20], Y_train[:-20]\n",
    "(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = FuzzyKNN(sensitivity=1, exp_bound=0.4)\n",
    "clf1.fit(X_train, Y_train)\n",
    "clf1.score(X_test, Y_test, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy Min-Max Classificator - IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = FuzzyMMC(sensitivity=1, exp_bound=0.4)\n",
    "clf2.fit(X_train, Y_train)\n",
    "clf2.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN algoritam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = KNeighborsClassifier(n_neighbors = 4)\n",
    "clf3.fit(X_train, Y_train)\n",
    "Y_pred = clf3.predict(X_test)\n",
    "accuracy = met.accuracy_score(Y_test, Y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WINE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------\n",
      "WINE DATASET: \n",
      "\n",
      "\n",
      "     Wine  Alcohol  Malic.acid   Ash   Acl   Mg  Phenols  Flavanoids  \\\n",
      "88      1    11.64        2.06  2.46  21.6   84     1.95        1.69   \n",
      "83      1    13.05        3.86  2.32  22.5   85     1.65        1.59   \n",
      "68      1    13.34        0.94  2.36  17.0  110     2.53        1.30   \n",
      "131     2    12.88        2.99  2.40  20.0  104     1.30        1.22   \n",
      "98      1    12.37        1.07  2.10  18.5   88     3.52        3.75   \n",
      "164     2    13.78        2.76  2.30  22.0   90     1.35        0.68   \n",
      "157     2    12.45        3.03  2.64  27.0   97     1.90        0.58   \n",
      "37      0    13.05        1.65  2.55  18.0   98     2.45        2.43   \n",
      "168     2    13.58        2.58  2.69  24.5  105     1.55        0.84   \n",
      "122     1    12.42        4.43  2.73  26.5  102     2.20        2.13   \n",
      "6       0    14.39        1.87  2.45  14.6   96     2.50        2.52   \n",
      "10      0    14.10        2.16  2.30  18.0  105     2.95        3.32   \n",
      "96      1    11.81        2.12  2.74  21.5  134     1.60        0.99   \n",
      "65      1    12.37        1.21  2.56  18.1   98     2.42        2.65   \n",
      "79      1    12.70        3.87  2.40  23.0  101     2.83        2.55   \n",
      "39      0    14.22        3.99  2.51  13.2  128     2.00        3.04   \n",
      "78      1    12.33        0.99  1.95  14.8  136     1.90        1.85   \n",
      "147     2    12.87        4.61  2.48  21.5   86     1.70        0.65   \n",
      "56      0    14.22        1.70  2.30  16.3  118     3.20        2.00   \n",
      "130     2    12.86        1.35  2.32  18.0  122     1.51        1.25   \n",
      "165     2    13.73        4.36  2.26  22.5   88     1.28        0.47   \n",
      "126     1    12.43        1.53  2.29  21.5   86     2.74        3.15   \n",
      "30      0    13.73        1.50  2.70  22.5  101     2.00        3.25   \n",
      "60      1    12.33        1.10  2.28  16.0  101     2.05        1.09   \n",
      "7       0    14.06        2.15  2.61  17.6  121     2.60        2.51   \n",
      "99      1    12.29        3.17  2.21  18.0   88     2.85        2.99   \n",
      "22      0    13.71        1.86  2.36  16.6  101     2.61        2.88   \n",
      "129     1    12.04        4.30  2.38  22.0   80     2.10        1.75   \n",
      "82      1    12.08        1.13  2.51  24.0   78     1.00        1.58   \n",
      "95      1    12.47        1.52  2.20  19.0  162     2.50        2.27   \n",
      "..    ...      ...         ...   ...   ...  ...      ...         ...   \n",
      "145     2    13.16        3.57  2.15  21.0  102     1.50        0.55   \n",
      "97      1    12.29        1.41  1.98  16.0   85     2.55        2.50   \n",
      "5       0    14.20        1.76  2.45  15.2  112     3.27        3.39   \n",
      "91      1    12.00        1.51  2.42  22.0   86     1.45        1.25   \n",
      "171     2    12.77        2.39  2.28  19.5   86     1.39        0.51   \n",
      "63      1    12.37        1.13  2.16  19.0   87     3.50        3.10   \n",
      "19      0    13.64        3.10  2.56  15.2  116     2.70        3.03   \n",
      "111     1    12.52        2.43  2.17  21.0   88     2.55        2.27   \n",
      "15      0    13.63        1.81  2.70  17.2  112     2.85        2.91   \n",
      "36      0    13.28        1.64  2.84  15.5  110     2.60        2.68   \n",
      "155     2    13.17        5.19  2.32  22.0   93     1.74        0.63   \n",
      "21      0    12.93        3.80  2.65  18.6  102     2.41        2.41   \n",
      "4       0    13.24        2.59  2.87  21.0  118     2.80        2.69   \n",
      "177     2    14.13        4.10  2.74  24.5   96     2.05        0.76   \n",
      "80      1    12.00        0.92  1.00  19.0   86     2.42        2.26   \n",
      "76      1    13.03        0.90  1.71  16.0   86     1.95        2.03   \n",
      "8       0    14.83        1.64  2.17  14.0   97     2.80        2.98   \n",
      "141     2    13.36        2.56  2.35  20.0   89     1.40        0.50   \n",
      "109     1    11.61        1.35  2.70  20.0   94     2.74        2.92   \n",
      "117     1    12.42        1.61  2.19  22.5  108     1.00        2.09   \n",
      "115     1    11.03        1.51  2.20  21.5   85     2.46        2.17   \n",
      "2       0    13.16        2.36  2.67  18.6  101     2.80        3.24   \n",
      "125     1    12.07        2.16  2.17  21.0   85     2.60        2.65   \n",
      "23      0    12.85        1.60  2.52  17.8   95     2.48        2.37   \n",
      "149     2    13.08        3.90  2.36  21.5  113     1.41        1.39   \n",
      "17      0    13.83        1.57  2.62  20.0  115     2.95        3.40   \n",
      "113     1    11.41        0.74  2.50  21.0   88     2.48        2.01   \n",
      "166     2    13.45        3.70  2.60  23.0  111     1.70        0.92   \n",
      "38      0    13.07        1.50  2.10  15.5   98     2.40        2.64   \n",
      "162     2    12.85        3.27  2.58  22.0  106     1.65        0.60   \n",
      "\n",
      "     Nonflavanoid.phenols  Proanth  Color.int   Hue    OD  Proline  \n",
      "88                   0.48     1.35   2.800000  0.00  2.75      680  \n",
      "83                   0.61     1.62   4.800000  0.84  2.01      515  \n",
      "68                   0.55     0.42   3.170000  1.02  1.93      750  \n",
      "131                  0.24     0.83   5.400000  0.74  1.42      530  \n",
      "98                   0.24     1.95   4.500000  1.04  2.77      660  \n",
      "164                  0.41     1.03   9.580000  0.70  1.68      615  \n",
      "157                  0.63     1.14   7.500000  0.67  1.73      880  \n",
      "37                   0.29     1.44   4.250000  1.12  2.51     1105  \n",
      "168                  0.39     1.54   8.660000  0.74  1.80      750  \n",
      "122                  0.43     1.71   2.080000  0.92  3.12      365  \n",
      "6                    0.30     1.98   5.250000  1.02  3.58     1290  \n",
      "10                   0.22     2.38   5.750000  1.25  3.17     1510  \n",
      "96                   0.14     1.56   2.500000  0.95  2.26      625  \n",
      "65                   0.37     2.08   4.600000  1.19  2.30      678  \n",
      "79                   0.43     1.95   2.570000  1.19  3.13      463  \n",
      "39                   0.20     2.08   5.100000  0.89  3.53      760  \n",
      "78                   0.35     2.76   3.400000  1.06  2.31      750  \n",
      "147                  0.47     0.86   7.650000  0.54  1.86      625  \n",
      "56                   0.26     2.03   6.380000  0.94  3.31      970  \n",
      "130                  0.21     0.94   4.100000  0.76  1.29      630  \n",
      "165                  0.52     1.15   6.620000  0.78  1.75      520  \n",
      "126                  0.39     1.77   3.940000  0.69  2.84      352  \n",
      "30                   0.29     2.38   5.700000  1.19  2.71     1285  \n",
      "60                   0.63     0.41   3.270000  1.25  1.67      680  \n",
      "7                    0.31     1.25   5.050000  1.06  3.58     1295  \n",
      "99                   0.45     2.81   2.300000  1.42  2.83      406  \n",
      "22                   0.27     1.69   3.800000  1.11  4.00     1035  \n",
      "129                  0.42     1.35   2.600000  0.79  2.57      580  \n",
      "82                   0.40     1.40   2.200000  1.31  2.72      630  \n",
      "95                   0.32     3.28   2.600000  1.16  2.63      937  \n",
      "..                    ...      ...        ...   ...   ...      ...  \n",
      "145                  0.43     1.30   4.000000  0.60  1.68      830  \n",
      "97                   0.29     1.77   2.900000  1.23  2.74      428  \n",
      "5                    0.34     1.97   6.750000  1.05  2.85     1450  \n",
      "91                   0.50     1.63   3.600000  1.05  2.65      450  \n",
      "171                  0.48     0.64   9.899999  0.57  1.63      470  \n",
      "63                   0.19     1.87   4.450000  1.22  2.87      420  \n",
      "19                   0.17     1.66   5.100000  0.96  3.36      845  \n",
      "111                  0.26     1.22   1.000000  0.90  2.78      325  \n",
      "15                   0.30     1.46   7.300000  1.28  2.88     1310  \n",
      "36                   0.34     1.36   4.600000  1.09  2.78      880  \n",
      "155                  0.61     1.55   7.900000  0.60  1.48      725  \n",
      "21                   0.25     1.98   4.500000  1.03  3.52      770  \n",
      "4                    0.39     1.82   4.320000  1.04  2.93      735  \n",
      "177                  0.56     1.35   9.200000  0.61  1.60      560  \n",
      "80                   0.30     1.43   2.500000  1.38  3.12      278  \n",
      "76                   0.24     1.46   4.600000  1.19  2.48      392  \n",
      "8                    0.29     1.98   5.200000  1.08  2.85     1045  \n",
      "141                  0.37     0.64   5.600000  0.70  2.47      780  \n",
      "109                  0.29     2.49   2.650000  0.96  3.26      680  \n",
      "117                  0.34     1.61   2.060000  1.06  2.96      345  \n",
      "115                  0.52     2.01   1.900000  1.71  2.87      407  \n",
      "2                    0.30     2.81   5.680000  1.03  3.17     1185  \n",
      "125                  0.37     1.35   2.760000  0.86  3.28      378  \n",
      "23                   0.26     1.46   3.930000  1.09  3.63     1015  \n",
      "149                  0.34     1.14   9.400000  0.57  1.33      550  \n",
      "17                   0.40     1.72   6.600000  1.13  2.57     1130  \n",
      "113                  0.42     1.44   3.080000  1.10  2.31      434  \n",
      "166                  0.43     1.46  10.680000  0.85  1.56      695  \n",
      "38                   0.28     1.37   3.700000  1.18  2.69     1020  \n",
      "162                  0.60     0.96   5.580000  0.87  2.11      570  \n",
      "\n",
      "[178 rows x 14 columns]\n",
      "[[0.84210526 0.1916996 ]\n",
      " [0.57105263 0.2055336 ]\n",
      " [0.56052632 0.3201581 ]\n",
      " [0.87894737 0.23913043]\n",
      " [0.58157895 0.36561265]\n",
      " [0.83421053 0.20158103]\n",
      " [0.88421053 0.22332016]\n",
      " [0.79736842 0.27865613]\n",
      " [1.         0.17786561]\n",
      " [0.74473684 0.12055336]\n",
      " [0.80789474 0.28063241]\n",
      " [0.81315789 0.14624506]\n",
      " [0.71578947 0.19565217]\n",
      " [0.97894737 0.19565217]\n",
      " [0.88157895 0.22332016]\n",
      " [0.68421053 0.21146245]\n",
      " [0.86052632 0.23320158]\n",
      " [0.73684211 0.16403162]\n",
      " [0.83157895 0.16798419]\n",
      " [0.68684211 0.46640316]\n",
      " [0.79736842 0.17588933]\n",
      " [0.5        0.60474308]\n",
      " [0.70526316 0.22134387]\n",
      " [0.47894737 0.16996047]\n",
      " [0.65       0.21146245]\n",
      " [0.53157895 0.25889328]\n",
      " [0.62105263 0.20355731]\n",
      " [0.59736842 0.19367589]\n",
      " [0.74736842 0.22924901]\n",
      " [0.78684211 0.18577075]\n",
      " [0.71052632 0.15019763]\n",
      " [0.67105263 0.18181818]\n",
      " [0.69736842 0.21541502]\n",
      " [0.71842105 0.15612648]\n",
      " [0.65263158 0.20948617]\n",
      " [0.64473684 0.21146245]\n",
      " [0.59210526 0.17786561]\n",
      " [0.53157895 0.1798419 ]\n",
      " [0.53684211 0.15019763]\n",
      " [0.83947368 0.64229249]\n",
      " [0.66578947 0.1916996 ]\n",
      " [0.62631579 0.61264822]\n",
      " [0.75       0.22727273]\n",
      " [0.58157895 0.64031621]\n",
      " [0.53157895 0.20355731]\n",
      " [0.83684211 0.65217391]\n",
      " [0.88157895 0.56324111]\n",
      " [0.75526316 0.18577075]\n",
      " [0.80789474 0.25296443]\n",
      " [0.76578947 0.19565217]\n",
      " [0.53157895 0.19565217]\n",
      " [0.73684211 0.1798419 ]\n",
      " [0.73421053 0.19960474]\n",
      " [0.72105263 0.22924901]\n",
      " [0.71315789 0.18379447]\n",
      " [0.66578947 0.19565217]\n",
      " [0.83947368 0.18972332]\n",
      " [0.59473684 0.243083  ]\n",
      " [0.70789474 0.13636364]\n",
      " [0.35263158 0.03952569]\n",
      " [0.34210526 0.07114625]\n",
      " [0.42368421 0.12252964]\n",
      " [0.69473684 0.10079051]\n",
      " [0.35263158 0.0770751 ]\n",
      " [0.3        0.14031621]\n",
      " [0.35263158 0.09288538]\n",
      " [0.54736842 0.05335968]\n",
      " [0.35263158 0.08498024]\n",
      " [0.60789474 0.03952569]\n",
      " [0.31052632 0.08893281]\n",
      " [0.33157895 0.17193676]\n",
      " [0.74473684 0.15217391]\n",
      " [0.64736842 0.18181818]\n",
      " [0.51578947 0.18379447]\n",
      " [0.24473684 0.06916996]\n",
      " [0.16578947 0.22529644]\n",
      " [0.52631579 0.03162055]\n",
      " [0.21315789 0.42490119]\n",
      " [0.34210526 0.04940711]\n",
      " [0.43947368 0.61857708]\n",
      " [0.25526316 0.03557312]\n",
      " [0.44473684 0.21146245]\n",
      " [0.27631579 0.0770751 ]\n",
      " [0.53157895 0.61660079]\n",
      " [0.21315789 0.02964427]\n",
      " [0.43157895 0.04743083]\n",
      " [0.29736842 0.17193676]\n",
      " [0.16315789 0.18379447]\n",
      " [0.16052632 0.26086957]\n",
      " [0.27631579 0.11660079]\n",
      " [0.27631579 0.21541502]\n",
      " [0.25526316 0.15217391]\n",
      " [0.43684211 0.15612648]\n",
      " [0.33157895 0.41304348]\n",
      " [0.15526316 0.24703557]\n",
      " [0.37894737 0.1541502 ]\n",
      " [0.20526316 0.27272727]\n",
      " [0.33157895 0.13241107]\n",
      " [0.35263158 0.06521739]\n",
      " [0.33157895 0.48023715]\n",
      " [0.27631579 0.26482213]\n",
      " [0.41315789 0.11857708]\n",
      " [0.34473684 0.33794466]\n",
      " [0.20789474 0.19367589]\n",
      " [0.38947368 0.19565217]\n",
      " [0.36578947 0.35770751]\n",
      " [0.32105263 0.19565217]\n",
      " [0.44473684 0.19960474]\n",
      " [0.31315789 0.10869565]\n",
      " [0.15263158 0.12055336]\n",
      " [0.11315789 0.59288538]\n",
      " [0.39210526 0.33399209]\n",
      " [0.19210526 0.38339921]\n",
      " [0.1        0.        ]\n",
      " [0.27631579 0.1284585 ]\n",
      " [0.         0.15217391]\n",
      " [0.20789474 0.14426877]\n",
      " [0.36578947 0.17193676]\n",
      " [0.45789474 0.53162055]\n",
      " [0.25526316 0.53162055]\n",
      " [0.11052632 0.32806324]\n",
      " [0.13947368 0.25889328]\n",
      " [0.36578947 0.72924901]\n",
      " [0.53157895 1.        ]\n",
      " [0.22105263 0.7055336 ]\n",
      " [0.27368421 0.28063241]\n",
      " [0.36842105 0.15612648]\n",
      " [0.2        0.27470356]\n",
      " [0.35263158 0.17588933]\n",
      " [0.26578947 0.70355731]\n",
      " [0.48157895 0.12055336]\n",
      " [0.48684211 0.44466403]\n",
      " [0.46842105 0.31027668]\n",
      " [0.43947368 0.55533597]\n",
      " [0.38947368 0.09881423]\n",
      " [0.41315789 0.33992095]\n",
      " [0.32105263 0.78656126]\n",
      " [0.39473684 0.94268775]\n",
      " [0.64736842 0.56324111]\n",
      " [0.47631579 0.43873518]\n",
      " [0.5        0.40909091]\n",
      " [0.61315789 0.35968379]\n",
      " [0.65526316 0.48023715]\n",
      " [0.68157895 0.83201581]\n",
      " [0.32105263 0.62055336]\n",
      " [0.56052632 0.55928854]\n",
      " [0.75       0.84980237]\n",
      " [0.48421053 0.76482213]\n",
      " [0.60263158 0.49407115]\n",
      " [0.53947368 0.62450593]\n",
      " [0.65       0.47035573]\n",
      " [0.46315789 0.38142292]\n",
      " [0.54736842 0.22924901]\n",
      " [0.57894737 0.50592885]\n",
      " [0.40789474 0.10869565]\n",
      " [0.56315789 0.87944664]\n",
      " [0.73947368 0.66798419]\n",
      " [0.37368421 0.45256917]\n",
      " [0.87105263 0.18577075]\n",
      " [0.64473684 0.18379447]\n",
      " [0.35       0.61067194]\n",
      " [0.7        0.49802372]\n",
      " [0.47894737 0.5       ]\n",
      " [0.50789474 0.53557312]\n",
      " [0.72368421 0.39920949]\n",
      " [0.71052632 0.71541502]\n",
      " [0.63684211 0.58498024]\n",
      " [0.47105263 0.51976285]\n",
      " [0.67105263 0.36363636]\n",
      " [0.62368421 0.76284585]\n",
      " [0.30789474 0.45256917]\n",
      " [0.45789474 0.32608696]\n",
      " [0.82368421 0.34980237]\n",
      " [0.70526316 0.97035573]\n",
      " [0.62368421 0.62648221]\n",
      " [0.58947368 0.69960474]\n",
      " [0.56315789 0.36561265]\n",
      " [0.81578947 0.66403162]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n---------------------------------\\nWINE DATASET: \\n\\n\")\n",
    "#df = pd.read_csv('wine.csv', header=None, names=['sepal length', 'sepal width', 'petal length', 'petal width', 'class'])\n",
    "#df = df[~(df['class']=='Iris-virginica')]\n",
    "df = pd.read_csv('wine.csv')\n",
    "df.head()\n",
    "\n",
    "df.replace(to_replace=1, value=0, inplace=True)\n",
    "df.replace(to_replace=2, value=1, inplace=True)\n",
    "df.replace(to_replace=3, value=2, inplace=True)\n",
    "\n",
    "print(df.sample(frac=1))\n",
    "\n",
    "X_train = df[['Alcohol', 'Malic.acid']].values\n",
    "Y_train = df['Wine'].values\n",
    "_max = np.max(X_train, axis=0)\n",
    "_min = np.min(X_train, axis=0)\n",
    "X_train = (X_train - _min) / (_max - _min)\n",
    "print(X_train)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, Y_test = X_train[-20:], Y_train[-20:]\n",
    "X_train, Y_train = X_train[:-20], Y_train[:-20]\n",
    "(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy Min-Max Classificator - WINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = FuzzyMMC(sensitivity=1, exp_bound=0.4)\n",
    "clf2.fit(X_train, Y_train)\n",
    "clf2.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritam koji koristi KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = FuzzyKNN(sensitivity=1, exp_bound=0.4)\n",
    "clf1.fit(X_train, Y_train)\n",
    "clf1.score(X_test, Y_test, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN algoritam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = KNeighborsClassifier(n_neighbors = 4)\n",
    "clf3.fit(X_train, Y_train)\n",
    "Y_pred = clf3.predict(X_test)\n",
    "accuracy = met.accuracy_score(Y_test, Y_pred)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
